{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkU5Uq_wFCrj"
   },
   "source": [
    "# QSAR Model Development  \n",
    "\n",
    "## Overview  \n",
    "\n",
    "This notebook performs the development and evaluation of quantitative structure–activity relationship (QSAR) models using the generated molecular descriptor dataset.\n",
    "\n",
    "The descriptor matrix is processed and partitioned into training and testing subsets prior to model construction. Machine learning regression algorithms are applied to establish relationships between molecular features and biological activity.\n",
    "\n",
    "Model performance is evaluated using standard regression metrics, including the coefficient of determination (R²) and root mean squared error (RMSE). Cross-validation is employed to assess model robustness and generalisability.\n",
    "\n",
    "This stage completes the computational pipeline by translating molecular descriptors into predictive models suitable for virtual screening and structure–activity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sxl8NkCO5eWl"
   },
   "source": [
    "**Step 1: Import Required Libraries**\n",
    "\n",
    "This section imports the essential libraries for data handling, feature preprocessing, model training, evaluation, and visualisation within the QSAR modelling workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3398,
     "status": "ok",
     "timestamp": 1762664883571,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "oDKZqHz-5fOB"
   },
   "outputs": [],
   "source": [
    "# === Core scientific libraries ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import pinv  # Used in leverage/applicability domain calculations\n",
    "\n",
    "# === Visualisation ===\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# === Scikit-learn: preprocessing & feature selection ===\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold, RFECV\n",
    "\n",
    "# === Scikit-learn: model training & evaluation ===\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    cross_val_predict,\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split\n",
    ")\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKmurQlwGyBV"
   },
   "source": [
    "### **Step 2: Load Required Input Files**\n",
    "\n",
    "This step loads the curated bioactivity dataset together with the descriptor matrices corresponding to the selected molecular fingerprint. These files are required for model training, validation, and prediction.\n",
    "\n",
    "**Required files:**\n",
    "- `<Fingerprint>.csv` — Descriptor matrix for training compounds  \n",
    "- `<Fingerprint>_Analogues.csv` — Descriptor matrix for external prediction set  \n",
    "- `bioactivity_dataset_curated.csv` — Curated bioactivity dataset  \n",
    "\n",
    "Ensure that the filenames correspond to the selected fingerprint type before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 36886,
     "status": "ok",
     "timestamp": 1762664920498,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "jVnh3tif-eOW",
    "outputId": "6fe00ab9-74db-459f-ac08-226f1f0f54f4"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define fingerprint type (must match descriptor file prefix)\n",
    "fingerprint = \"Substructure\"  # Adjust as needed\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "\n",
    "# Define file paths\n",
    "train_descriptor_file = DATA_DIR / f\"{fingerprint}.csv\"\n",
    "external_descriptor_file = DATA_DIR / f\"{fingerprint}_Analogues.csv\"\n",
    "bioactivity_file = DATA_DIR / \"bioactivity_dataset_curated.csv\"\n",
    "\n",
    "# Validate existence\n",
    "for file in [train_descriptor_file, external_descriptor_file, bioactivity_file]:\n",
    "    assert file.exists(), f\"Missing required file: {file}\"\n",
    "\n",
    "# Load datasets\n",
    "X_train_descriptors = pd.read_csv(train_descriptor_file)\n",
    "X_external_descriptors = pd.read_csv(external_descriptor_file)\n",
    "bioactivity_df = pd.read_csv(bioactivity_file)\n",
    "\n",
    "print(\"All required datasets loaded successfully.\")\n",
    "print(f\"Training descriptor shape: {X_train_descriptors.shape}\")\n",
    "print(f\"External descriptor shape: {X_external_descriptors.shape}\")\n",
    "print(f\"Bioactivity dataset shape: {bioactivity_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1762664920649,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "5TMU8mHRH2q_"
   },
   "source": [
    "**Step 3: Merge Descriptor and Bioactivity Data**\n",
    "\n",
    "The descriptor matrix is merged with the curated bioactivity dataset using the compound identifier.  \n",
    "The combined dataset is then partitioned into internal (training) and external (test) sets prior to model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1762664920668,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "qSUEbRWFH2nN",
    "outputId": "614fba1f-66c1-4f90-855e-72640ec89345"
   },
   "outputs": [],
   "source": [
    "# Define fingerprint type (must match descriptor file prefix)\n",
    "fingerprint = \"AtomPairs2DCount\"  # Adjust as needed\n",
    "\n",
    "# Load descriptor and bioactivity datasets\n",
    "desc_all = pd.read_csv(f\"{fingerprint}.csv\")            # Training descriptors\n",
    "bioactivity = pd.read_csv(\"bioactivity_dataset_curated.csv\")  # Curated activity data\n",
    "tq_desc = pd.read_csv(f\"{fingerprint}_Analogues.csv\")   # External prediction descriptors\n",
    "\n",
    "# Merge descriptors with bioactivity on compound ID\n",
    "df = pd.merge(desc_all, bioactivity, on=\"ID\")\n",
    "\n",
    "print(f\"Merged dataset shape: {df.shape}\")\n",
    "\n",
    "# Split dataset (80/20 ratio)\n",
    "int_set, ext_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate descriptors and activity\n",
    "int_desc = int_set.drop(columns=[\"ID\", \"pIC50\"]).reset_index(drop=True)\n",
    "ext_desc = ext_set.drop(columns=[\"ID\", \"pIC50\"]).reset_index(drop=True)\n",
    "tq_desc  = tq_desc.drop(columns=\"ID\").reset_index(drop=True)\n",
    "\n",
    "# Keep numeric descriptor columns only\n",
    "descriptor_cols = int_desc.select_dtypes(include=[np.number]).columns\n",
    "int_desc = int_desc[descriptor_cols]\n",
    "ext_desc = ext_desc[descriptor_cols]\n",
    "\n",
    "# Define response variables\n",
    "y_int = int_set[\"pIC50\"].reset_index(drop=True)\n",
    "y_ext = ext_set[\"pIC50\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Internal descriptor shape:\", int_desc.shape)\n",
    "print(\"External descriptor shape:\", ext_desc.shape)\n",
    "print(\"Internal activity length:\", len(y_int))\n",
    "print(\"External activity length:\", len(y_ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnG1756jIhoB"
   },
   "source": [
    "**Step 4: Descriptor Preprocessing and Feature Selection**\n",
    "\n",
    "Descriptors are cleaned, standardised, and filtered to remove redundant or non-informative variables.  \n",
    "\n",
    "Feature selection is performed in multiple stages:\n",
    "1. Removal of constant and missing-value descriptors  \n",
    "2. Standard scaling (fit on internal set only)  \n",
    "3. High-correlation filtering  \n",
    "4. Variance threshold filtering  \n",
    "5. Activity-correlation filtering  \n",
    "6. Hybrid feature selection (Random Forest ranking + RFECV)\n",
    "7. Final train-test split\n",
    "\n",
    "All preprocessing steps are fitted exclusively on the internal training set to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Defensive cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1762665026283,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "awOIlnAdH2J1",
    "outputId": "c44e09da-e24c-498a-9297-c10c53a81472"
   },
   "outputs": [],
   "source": [
    "int0 = (\n",
    "    int_desc\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .loc[:, lambda df: df.std() > 0]\n",
    "    .dropna(axis=0)\n",
    ")\n",
    "\n",
    "clean_cols = int0.columns\n",
    "print(f\"After defensive cleaning → shape: {int0.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Scale all sets with same scaler (fit on internal only)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(int0)\n",
    "\n",
    "X_int_scaled = pd.DataFrame(\n",
    "    scaler.transform(int0),\n",
    "    columns=clean_cols,\n",
    "    index=int0.index\n",
    ")\n",
    "\n",
    "X_ext_scaled = pd.DataFrame(\n",
    "    scaler.transform(ext_desc[clean_cols]),\n",
    "    columns=clean_cols,\n",
    "    index=ext_desc.index\n",
    ")\n",
    "\n",
    "X_tq_scaled = pd.DataFrame(\n",
    "    scaler.transform(tq_desc[clean_cols]),\n",
    "    columns=clean_cols,\n",
    "    index=tq_desc.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Drop high-correlation descriptors (>|0.95|)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X_int_scaled.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "drop1 = [c for c in upper.columns if any(upper[c] > 0.95)]\n",
    "\n",
    "X_int_filt = X_int_scaled.drop(columns=drop1)\n",
    "X_ext_filt = X_ext_scaled.drop(columns=drop1)\n",
    "X_tq_filt  = X_tq_scaled.drop(columns=drop1)\n",
    "\n",
    "print(f\"After high-correlation filter (>|0.95|): {X_int_filt.shape[1]} descriptors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Variance filter (threshold = 0.01)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = VarianceThreshold(threshold=0.01).fit(X_int_filt)\n",
    "\n",
    "keep_cols = X_int_filt.columns[vt.get_support()]\n",
    "\n",
    "X_int_var = pd.DataFrame(\n",
    "    vt.transform(X_int_filt),\n",
    "    columns=keep_cols,\n",
    "    index=X_int_filt.index\n",
    ")\n",
    "\n",
    "X_ext_var = pd.DataFrame(\n",
    "    vt.transform(X_ext_filt),\n",
    "    columns=keep_cols,\n",
    "    index=X_ext_filt.index\n",
    ")\n",
    "\n",
    "X_tq_var = pd.DataFrame(\n",
    "    vt.transform(X_tq_filt),\n",
    "    columns=keep_cols,\n",
    "    index=X_tq_filt.index\n",
    ")\n",
    "\n",
    "print(f\"After VarianceThreshold(0.01): {X_int_var.shape[1]} descriptors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Drop descriptors highly correlated with activity (|r| > 0.7)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_with_y = X_int_var.apply(lambda c: c.corr(y_int))\n",
    "drop_cols = r_with_y[r_with_y.abs() > 0.7].index.tolist()\n",
    "\n",
    "X_int_red = X_int_var.drop(columns=drop_cols, errors=\"ignore\")\n",
    "X_ext_red = X_ext_var.drop(columns=drop_cols, errors=\"ignore\")\n",
    "X_tq_red  = X_tq_var.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "print(f\"After activity-correlation filter (|r|>0.7): {X_int_red.shape[1]} descriptors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Hybrid feature selection: RF ranking → RFECV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Stage 1: Random Forest ranking (Top-K selection) -----\n",
    "top_k = min(50, X_int_red.shape[1])\n",
    "\n",
    "rf_rank = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_rank.fit(X_int_red, y_int)\n",
    "\n",
    "importances = rf_rank.feature_importances_\n",
    "order = np.argsort(importances)[::-1]\n",
    "topK_idx = order[:top_k]\n",
    "topK_cols = X_int_red.columns[topK_idx]\n",
    "\n",
    "X_int_topK = X_int_red[topK_cols].copy()\n",
    "\n",
    "print(f\"Stage 1 → Top-{top_k} preselected features\")\n",
    "\n",
    "# ----- Stage 2: RFECV on Top-K -----\n",
    "cv10 = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "rf_base = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=rf_base,\n",
    "    step=3,\n",
    "    cv=cv10,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    min_features_to_select=30\n",
    ")\n",
    "\n",
    "rfecv.fit(X_int_topK, y_int)\n",
    "\n",
    "sel_mask = rfecv.support_\n",
    "sel_cols = X_int_topK.columns[sel_mask]\n",
    "\n",
    "print(f\"RFECV selected {len(sel_cols)} optimal features\")\n",
    "\n",
    "# Final selected feature matrices\n",
    "X_int_sel = X_int_red[sel_cols].copy()\n",
    "X_ext_sel = X_ext_red[sel_cols].copy()\n",
    "X_tq_sel  = X_tq_red[sel_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Final train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_int_sel,\n",
    "    y_int,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training activity length: {y_train.shape}\")\n",
    "print(f\"Test activity length: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYpsW0GR2XUc"
   },
   "source": [
    "**Step 5: Hyperparameter Optimisation and Model Evaluation**\n",
    "\n",
    "Randomised hyperparameter search is performed to optimise the Random Forest regression model.  \n",
    "\n",
    "Model performance is evaluated using:\n",
    "- Coefficient of determination (R²)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "\n",
    "Performance is assessed on:\n",
    "1. Training set  \n",
    "2. Independent test set  \n",
    "3. Cross-validation predictions (internal robustness assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86976,
     "status": "ok",
     "timestamp": 1762665113262,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "G7mv5pVaH2F7",
    "outputId": "603f132f-c32a-4b52-a7e7-34d296b4df8a"
   },
   "outputs": [],
   "source": [
    "# === Hyperparameter Tuning ===\n",
    "\n",
    "# Define parameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 20, 30],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.5],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"Best Random Forest parameters:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# === 1. Training Set Performance ===\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "\n",
    "# === 2. Test Set Performance ===\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "# === 3. Cross-Validation Performance (Internal Set) ===\n",
    "cv10 = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "y_cv_pred = cross_val_predict(best_model, X_int_sel, y_int, cv=cv10)\n",
    "\n",
    "r2_cv = r2_score(y_int, y_cv_pred)\n",
    "rmse_cv = np.sqrt(mean_squared_error(y_int, y_cv_pred))\n",
    "\n",
    "# === Report Results ===\n",
    "print(\"\\nModel Performance Summary\")\n",
    "print(\"--------------------------\")\n",
    "print(f\"Training Set   → R²: {r2_train:.3f} | RMSE: {rmse_train:.3f}\")\n",
    "print(f\"Test Set       → R²: {r2_test:.3f} | RMSE: {rmse_test:.3f}\")\n",
    "print(f\"Cross-Validation (10-fold) → R²: {r2_cv:.3f} | RMSE: {rmse_cv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtZYaPb32o3F"
   },
   "source": [
    "**Step 6: Independent 10-Fold Cross-Validation (Q²CV)**\n",
    "\n",
    "A manual 10-fold cross-validation procedure is performed using the optimised Random Forest model.  \n",
    "\n",
    "For each fold:\n",
    "- The model is trained on 90% of the internal data  \n",
    "- Predictions are generated for the remaining 10%  \n",
    "- R² and RMSE are computed  \n",
    "\n",
    "The mean R² across folds is reported as Q²CV, representing internal predictive robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6634,
     "status": "ok",
     "timestamp": 1762665119904,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "au6qmdVmH2Ca",
    "outputId": "fb99116c-8ccc-497f-e400-7a095e04d975"
   },
   "outputs": [],
   "source": [
    "# === 10-Fold Cross-Validation (Independent Evaluation) ===\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "rmse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_int_sel):\n",
    "\n",
    "    X_fold_train = X_int_sel.iloc[train_idx]\n",
    "    X_fold_val   = X_int_sel.iloc[val_idx]\n",
    "\n",
    "    y_fold_train = y_int.iloc[train_idx]\n",
    "    y_fold_val   = y_int.iloc[val_idx]\n",
    "\n",
    "    # Re-instantiate model using best hyperparameters\n",
    "    model = RandomForestRegressor(\n",
    "        **random_search.best_params_,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "    y_fold_pred = model.predict(X_fold_val)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_fold_val, y_fold_pred))\n",
    "    r2   = r2_score(y_fold_val, y_fold_pred)\n",
    "\n",
    "    rmse_scores.append(rmse)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Compute mean Q²CV and RMSECV\n",
    "q2_cv   = np.mean(r2_scores)\n",
    "rmse_cv = np.mean(rmse_scores)\n",
    "\n",
    "print(\"\\n10-Fold Cross-Validation Results\")\n",
    "print(\"----------------------------------\")\n",
    "print(f\"Q²CV (mean R²): {q2_cv:.3f}\")\n",
    "print(f\"RMSECV (mean RMSE): {rmse_cv:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hsDFexn3NWs"
   },
   "source": [
    "**Step 7: External Prediction and Performance Summary**\n",
    "\n",
    "The optimised Random Forest model is applied to the external validation set.  \n",
    "\n",
    "Performance is evaluated using:\n",
    "- R² (external test set)\n",
    "- RMSE (external test set)\n",
    "- Q²Ext (external predictive coefficient)\n",
    "\n",
    "A consolidated performance table summarises all validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1762665119977,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "IyY97SXXwLiV",
    "outputId": "a2466c00-3879-4f12-d4f5-949eb5996ff3"
   },
   "outputs": [],
   "source": [
    "# === External Predictions ===\n",
    "y_ext_pred = best_model.predict(X_ext_sel)\n",
    "\n",
    "R2_Ext = r2_score(y_ext, y_ext_pred)\n",
    "RMSE_Ext = np.sqrt(mean_squared_error(y_ext, y_ext_pred))\n",
    "\n",
    "# Q²Ext calculation (external predictive squared correlation)\n",
    "Q2_Ext = 1 - (\n",
    "    ((y_ext - y_ext_pred) ** 2).sum() /\n",
    "    ((y_ext - y_train.mean()) ** 2).sum()\n",
    ")\n",
    "\n",
    "print(\"\\nExternal Validation Results\")\n",
    "print(\"----------------------------\")\n",
    "print(f\"R²_Ext  : {R2_Ext:.3f}\")\n",
    "print(f\"RMSE_Ext: {RMSE_Ext:.3f}\")\n",
    "print(f\"Q²_Ext  : {Q2_Ext:.3f}\")\n",
    "\n",
    "# === Performance Summary Table ===\n",
    "performance_metrics = pd.DataFrame({\n",
    "    \"Model\": [fingerprint],\n",
    "    \"R²_Train\": [r2_train],\n",
    "    \"RMSE_Train\": [rmse_train],\n",
    "    \"R²_Test\": [r2_test],\n",
    "    \"RMSE_Test\": [rmse_test],\n",
    "    \"Q²_CV\": [q2_cv],\n",
    "    \"RMSE_CV\": [rmse_cv],\n",
    "    \"Q²_Ext\": [Q2_Ext],\n",
    "    \"RMSE_Ext\": [RMSE_Ext],\n",
    "    \"Δ(R²_Test − Q²_CV)\": [r2_test - q2_cv],\n",
    "    \"Δ(R²_Test − Q²_Ext)\": [r2_test - Q2_Ext],\n",
    "    \"Selected_Descriptors\": [len(sel_cols)]\n",
    "}).round(3)\n",
    "\n",
    "performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "---e8YI4trg_"
   },
   "source": [
    "**Step 8: Test Set Predictive Coefficient (Q²Test)**\n",
    "\n",
    "The predictive squared correlation coefficient (Q²Test) is calculated for the independent test set using the training set mean as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1762665120029,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "XwsWd2XWthFS",
    "outputId": "9bbabfda-c8db-40e6-bd2f-0418c8dce700"
   },
   "outputs": [],
   "source": [
    "# === Q²Test Calculation (Independent Test Set Predictivity) ===\n",
    "Q2_Test = 1 - (\n",
    "    ((y_test - y_pred_test) ** 2).sum() /\n",
    "    ((y_test - y_train.mean()) ** 2).sum()\n",
    ")\n",
    "\n",
    "print(f\"Q²Test (independent test set): {Q2_Test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnkMRkOY4iYF"
   },
   "source": [
    "**Step 9: Y-Randomisation (Y-Scrambling) Test**\n",
    "\n",
    "To assess the possibility of chance correlation, the response variable (activity) is randomly shuffled and the modelling procedure is repeated multiple times.\n",
    "\n",
    "If the model is statistically robust, the resulting R² and Q² values from randomised models should be significantly lower than those obtained from the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27711,
     "status": "ok",
     "timestamp": 1762665147747,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "M_U-gSvRH14b"
   },
   "outputs": [],
   "source": [
    "# === Y-Randomisation Test ===\n",
    "\n",
    "r2_randomised = []\n",
    "q2_randomised = []\n",
    "n_iterations = 100\n",
    "\n",
    "for seed in range(n_iterations):\n",
    "\n",
    "    # Shuffle activity values\n",
    "    y_shuffled = y_int.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(\n",
    "        X_int_sel,\n",
    "        y_shuffled,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Use optimised hyperparameters\n",
    "    model_rand = RandomForestRegressor(\n",
    "        **random_search.best_params_,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model_rand.fit(X_train_rand, y_train_rand)\n",
    "\n",
    "    y_pred_rand = model_rand.predict(X_test_rand)\n",
    "\n",
    "    r2_rand = r2_score(y_test_rand, y_pred_rand)\n",
    "\n",
    "    q2_rand = 1 - (\n",
    "        ((y_test_rand - y_pred_rand) ** 2).sum() /\n",
    "        ((y_test_rand - y_train_rand.mean()) ** 2).sum()\n",
    "    )\n",
    "\n",
    "    r2_randomised.append(r2_rand)\n",
    "    q2_randomised.append(q2_rand)\n",
    "\n",
    "# === Summary Statistics ===\n",
    "mean_r2_rand = np.mean(r2_randomised)\n",
    "mean_q2_rand = np.mean(q2_randomised)\n",
    "\n",
    "print(\"\\nY-Randomisation Results\")\n",
    "print(\"------------------------\")\n",
    "print(f\"Mean R² (randomised): {mean_r2_rand:.3f}\")\n",
    "print(f\"Mean Q² (randomised): {mean_q2_rand:.3f}\")\n",
    "print(f\"Original R²_Test: {r2_test:.3f}\")\n",
    "print(f\"Original Q²_Test: {Q2_Test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADEZ76-U5W6F"
   },
   "source": [
    "**Step 10: Model Performance Visualisation**\n",
    "\n",
    "Two validation plots are generated:\n",
    "\n",
    "(a) Predicted vs Experimental pIC₅₀ for internal and external sets  \n",
    "(b) Y-randomisation validation (R² vs Q² comparison)\n",
    "\n",
    "Figures are exported in high resolution for reporting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "executionInfo": {
     "elapsed": 4747,
     "status": "ok",
     "timestamp": 1762665152523,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "hBpV1o88H10K",
    "outputId": "04fc0faf-167a-4470-c0ec-fde101a0b155"
   },
   "outputs": [],
   "source": [
    "# ---------- Matplotlib Configuration ----------\n",
    "mpl.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"STIXGeneral\",\n",
    "    \"mathtext.fontset\": \"stix\",\n",
    "    \"mathtext.default\": \"regular\",\n",
    "})\n",
    "\n",
    "# ---------- Create Figure ----------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# =======================\n",
    "# (a) Predicted vs Experimental\n",
    "# =======================\n",
    "ax1 = axes[0]\n",
    "\n",
    "ax1.scatter(\n",
    "    y_test, y_pred_test,\n",
    "    c=\"#2ca02c\",\n",
    "    label=\"Internal Test Set\",\n",
    "    edgecolor=\"black\",\n",
    "    s=60,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "ax1.scatter(\n",
    "    y_ext, y_ext_pred,\n",
    "    c=\"#d62728\",\n",
    "    label=\"External Set\",\n",
    "    edgecolor=\"black\",\n",
    "    s=60,\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "min_val = min(\n",
    "    y_test.min(), y_ext.min(),\n",
    "    y_pred_test.min(), y_ext_pred.min()\n",
    ")\n",
    "\n",
    "max_val = max(\n",
    "    y_test.max(), y_ext.max(),\n",
    "    y_pred_test.max(), y_ext_pred.max()\n",
    ")\n",
    "\n",
    "# Ideal prediction line\n",
    "ax1.plot([min_val, max_val], [min_val, max_val],\n",
    "         linestyle=\"--\", linewidth=2, label=\"Ideal Prediction\")\n",
    "\n",
    "ax1.set_xlabel(r'$\\mathbf{Experimental\\ pIC}_{50}$', fontsize=18)\n",
    "ax1.set_ylabel(r'$\\mathbf{Predicted\\ pIC}_{50}$', fontsize=18)\n",
    "\n",
    "ax1.set_xlim(min_val - 0.2, max_val + 0.2)\n",
    "ax1.set_ylim(min_val - 0.2, max_val + 0.2)\n",
    "\n",
    "ax1.legend(frameon=False, fontsize=14)\n",
    "ax1.tick_params(axis='both', direction='in', length=6, width=1.5)\n",
    "ax1.text(0.02, 0.98, '(a)', transform=ax1.transAxes,\n",
    "         fontsize=15, va='top', fontweight='bold')\n",
    "\n",
    "# =======================\n",
    "# (b) Y-Randomisation Plot\n",
    "# =======================\n",
    "ax2 = axes[1]\n",
    "\n",
    "ax2.scatter(\n",
    "    q2_randomised,\n",
    "    r2_randomised,\n",
    "    color='#d62728',\n",
    "    s=60,\n",
    "    alpha=0.8,\n",
    "    edgecolor='black',\n",
    "    label='Y-Scrambled Models'\n",
    ")\n",
    "\n",
    "ax2.scatter(\n",
    "    Q2_Test,\n",
    "    r2_test,\n",
    "    color='#2ca02c',\n",
    "    s=90,\n",
    "    edgecolor='black',\n",
    "    label='Actual Model'\n",
    ")\n",
    "\n",
    "ax2.set_xlabel(r'$\\mathbf{Q^2}$', fontsize=18)\n",
    "ax2.set_ylabel(r'$\\mathbf{R^2}$', fontsize=18)\n",
    "\n",
    "ax2.set_xlim(-1.0, 1.5)\n",
    "ax2.set_ylim(-1.0, 1.5)\n",
    "\n",
    "ax2.legend(frameon=False, fontsize=14)\n",
    "ax2.tick_params(axis='both', direction='in', length=6, width=1.5)\n",
    "ax2.text(0.02, 0.98, '(b)', transform=ax2.transAxes,\n",
    "         fontsize=15, va='top', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ---------- Save Figures ----------\n",
    "for ext in [\"png\", \"svg\", \"pdf\"]:\n",
    "    fname = f\"QSAR_Model_Validation.{ext}\"\n",
    "    fig.savefig(fname, dpi=600, bbox_inches='tight', facecolor='white')\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLjgg0mj6mBI"
   },
   "source": [
    "**Step 11: Applicability Domain Assessment (Williams Plot)**\n",
    "\n",
    "The Williams plot is generated to evaluate the model's applicability domain (AD).\n",
    "\n",
    "Leverage values identify structurally influential compounds, while standardised residuals detect response outliers.\n",
    "\n",
    "Compounds are considered:\n",
    "- Influential if leverage > h*\n",
    "- Outliers if |standardised residual| > 3\n",
    "\n",
    "The leverage threshold is defined as:\n",
    "\n",
    "\\[\n",
    "h^* = \\frac{3(p+1)}{n}\n",
    "\\]\n",
    "\n",
    "where *p* is the number of selected descriptors and *n* is the number of training compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "executionInfo": {
     "elapsed": 2085,
     "status": "ok",
     "timestamp": 1762665154633,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "G9Dr3NzhH1wh",
    "outputId": "3676c521-a9d0-4f57-e6d7-eaf9658d01c1"
   },
   "outputs": [],
   "source": [
    "# ---------- Matplotlib Configuration ----------\n",
    "mpl.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"STIXGeneral\",\n",
    "    \"mathtext.fontset\": \"stix\",\n",
    "    \"mathtext.default\": \"regular\",\n",
    "})\n",
    "\n",
    "# ---------- Keep IDs aligned ----------\n",
    "train_ids = int_set[\"ID\"].reset_index(drop=True).astype(str)\n",
    "test_ids  = ext_set[\"ID\"].reset_index(drop=True).astype(str)\n",
    "\n",
    "# ---------- Prepare NumPy matrices ----------\n",
    "X_train_np = X_train.to_numpy()\n",
    "X_test_np  = X_test.to_numpy()\n",
    "\n",
    "# === Step 1: Leverage Calculation ===\n",
    "XT_X_inv = np.linalg.pinv(X_train_np.T @ X_train_np)\n",
    "\n",
    "H = X_train_np @ XT_X_inv @ X_train_np.T\n",
    "leverage_train = np.diag(H)\n",
    "\n",
    "leverage_test = np.array([x @ XT_X_inv @ x.T for x in X_test_np])\n",
    "\n",
    "# === Step 2: Leverage Threshold ===\n",
    "n, p = X_train_np.shape\n",
    "h_star = 3 * (p + 1) / n\n",
    "\n",
    "print(f\"Leverage threshold (h*): {h_star:.4f}\")\n",
    "\n",
    "# === Step 3: Standardised Residuals ===\n",
    "residuals_train = y_pred_train - y_train\n",
    "residuals_test  = y_pred_test - y_test\n",
    "\n",
    "std_train = np.std(residuals_train)\n",
    "std_test  = np.std(residuals_test)\n",
    "\n",
    "standardised_residuals_train = residuals_train / (std_train if std_train != 0 else 1.0)\n",
    "standardised_residuals_test  = residuals_test  / (std_test if std_test != 0 else 1.0)\n",
    "\n",
    "# === Step 4: AD Statistics ===\n",
    "outliers_train = np.where(np.abs(standardised_residuals_train) > 3)[0]\n",
    "influential_train = np.where(leverage_train > h_star)[0]\n",
    "\n",
    "outliers_test = np.where(np.abs(standardised_residuals_test) > 3)[0]\n",
    "influential_test = np.where(leverage_test > h_star)[0]\n",
    "\n",
    "print(\"\\nInternal Set:\")\n",
    "print(f\"Outliers (|residual| > 3): {len(outliers_train)}\")\n",
    "print(f\"Influential (leverage > h*): {len(influential_train)}\")\n",
    "\n",
    "print(\"\\nExternal Set:\")\n",
    "print(f\"Outliers (|residual| > 3): {len(outliers_test)}\")\n",
    "print(f\"Influential (leverage > h*): {len(influential_test)}\")\n",
    "\n",
    "# === Step 5: Williams Plot ===\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.scatter(\n",
    "    leverage_train,\n",
    "    standardised_residuals_train,\n",
    "    c='#2ca02c',\n",
    "    edgecolor='black',\n",
    "    s=60,\n",
    "    alpha=0.8,\n",
    "    label='Internal set'\n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    leverage_test,\n",
    "    standardised_residuals_test,\n",
    "    c='#d62728',\n",
    "    edgecolor='black',\n",
    "    s=60,\n",
    "    alpha=0.8,\n",
    "    label='External set'\n",
    ")\n",
    "\n",
    "# Threshold lines\n",
    "ax.axhline(3, color='blue', linewidth=1.5)\n",
    "ax.axhline(-3, color='blue', linewidth=1.5)\n",
    "ax.axvline(h_star, color='blue', linestyle='--', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel(r'$\\mathbf{Leverage}$', fontsize=18)\n",
    "ax.set_ylabel(r'$\\mathbf{Standardised\\ Residuals}$', fontsize=18)\n",
    "\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.set_xlim(left=0)\n",
    "\n",
    "ax.tick_params(axis='both', direction='in', length=6, width=1.5)\n",
    "ax.legend(frameon=False, fontsize=12)\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ---------- Save Figure ----------\n",
    "for ext in [\"png\", \"svg\", \"pdf\"]:\n",
    "    fname = f\"WilliamsPlot_{fingerprint}.{ext}\"\n",
    "    fig.savefig(fname, dpi=600, bbox_inches='tight', facecolor='white')\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN6u1Ols7YTn"
   },
   "source": [
    "**Step 12: External Library Prediction and Activity Classification**\n",
    "\n",
    "The trained QSAR model is applied to an external analogue library.  \n",
    "\n",
    "Predicted pIC₅₀ values are generated and compounds are categorised into:\n",
    "- Active (pIC₅₀ ≥ 6)\n",
    "- Intermediate (5 ≤ pIC₅₀ < 6)\n",
    "- Inactive (pIC₅₀ < 5)\n",
    "\n",
    "Predictions are appended to the original library file and exported for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "executionInfo": {
     "elapsed": 232083,
     "status": "ok",
     "timestamp": 1762665386734,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "awbIhbZSKDhy",
    "outputId": "dcd62f00-ad5f-4aea-9fb2-5dc3fefd7ad0"
   },
   "outputs": [],
   "source": [
    "**Distribution of TQ Analogues by Applicability Domain and Activity Class**\n",
    "# ---------- Bubble Plot of AD vs Activity Class ----------\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# ---------- Fonts ----------\n",
    "mpl.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"STIXGeneral\",\n",
    "    \"mathtext.fontset\": \"stix\",\n",
    "    \"mathtext.default\": \"regular\",\n",
    "})\n",
    "\n",
    "# ---------- Load dataset ----------\n",
    "file_path = 'Thymoquinone_Analogues_library.csv'\n",
    "counts = (\n",
    "    pd.read_csv(file_path)\n",
    "    .groupby(['AD_Flag', 'Activity_Class'])\n",
    "    .size()\n",
    "    .reset_index(name='Count')\n",
    ")\n",
    "\n",
    "# ---------- Map categories to numeric axes ----------\n",
    "x_map = {'Inside AD': 0, 'Outside AD': 1}\n",
    "y_map = {'Inactive': 0, 'Intermediate': 1, 'Active': 2}\n",
    "counts['x'] = counts['AD_Flag'].map(x_map)\n",
    "counts['y'] = counts['Activity_Class'].map(y_map)\n",
    "\n",
    "# ---------- Define bubble sizes ----------\n",
    "bubble_sizes = counts['Count'] * 20 + 200  # scale factor + base size\n",
    "\n",
    "# ---------- Custom colormap ----------\n",
    "cmap = LinearSegmentedColormap.from_list('green_blue_red', ['green', 'blue', 'red'])\n",
    "\n",
    "# ---------- Create plot ----------\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "scatter = ax.scatter(\n",
    "    counts['x'], counts['y'],\n",
    "    s=bubble_sizes,\n",
    "    c=counts['Count'],\n",
    "    cmap=cmap,\n",
    "    alpha=0.7,\n",
    "    edgecolors='black',\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# ---------- Annotate bubbles ----------\n",
    "for _, row in counts.iterrows():\n",
    "    ax.text(\n",
    "        row['x'], row['y'], int(row['Count']),\n",
    "        ha='center', va='center', fontsize=12, color='white'\n",
    "    )\n",
    "\n",
    "# ---------- Axes limits and labels ----------\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 2.5)\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Inside AD', 'Outside AD'], fontsize=16)\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_yticklabels(['Inactive', 'Intermediate', 'Active'], fontsize=16)\n",
    "ax.set_xlabel(r'$\\mathbf{Applicability\\ Domain\\ Status}$', fontsize=16)\n",
    "ax.set_ylabel(r'$\\mathbf{Activity\\ Class}$', fontsize=16)\n",
    "\n",
    "# ---------- Colorbar ----------\n",
    "cbar = fig.colorbar(scatter, ax=ax, pad=0.02)\n",
    "cbar.set_label('Number of Compounds', fontsize=16, fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# ---------- Styling ----------\n",
    "ax.grid(False)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor('black')\n",
    "    spine.set_linewidth(2.0)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# ---------- Save & Auto-Download ----------\n",
    "for ext in [\"png\", \"svg\", \"pdf\"]:\n",
    "    fname = f\"Bubble_AD_Activity.{ext}\"\n",
    "    fig.savefig(fname, dpi=600, bbox_inches='tight', facecolor='white')\n",
    "    files.download(fname)\n",
    "\n",
    "# ---------- Display and close ----------\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Int_cdyUAss2"
   },
   "source": [
    "**Step 13: Applicability Domain Assessment for External Library**\n",
    "\n",
    "Leverage values are computed for the external analogue library using the training-set hat matrix.  \n",
    "\n",
    "Compounds are classified as:\n",
    "- Inside AD (leverage ≤ h*)\n",
    "- Outside AD (leverage > h*)\n",
    "\n",
    "The leverage values and AD flags are appended to the prediction output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1762665386773,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "DNqBmhhh0vuP",
    "outputId": "2e7eb55e-3f39-4c64-8b04-3de492fce2e8"
   },
   "outputs": [],
   "source": [
    "# === Applicability Domain for External Library ===\n",
    "\n",
    "# Convert selected descriptor matrix to NumPy\n",
    "X_tq_np = X_tq_sel.to_numpy()\n",
    "\n",
    "# Compute leverage values\n",
    "leverage_tq = np.array([x @ XT_X_inv @ x.T for x in X_tq_np])\n",
    "\n",
    "# Assign AD classification\n",
    "ad_flag = np.where(leverage_tq > h_star, \"Outside AD\", \"Inside AD\")\n",
    "\n",
    "# Summary statistics\n",
    "n_inside = np.sum(ad_flag == \"Inside AD\")\n",
    "n_outside = np.sum(ad_flag == \"Outside AD\")\n",
    "\n",
    "print(\"\\nApplicability Domain Summary\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"Inside AD : {n_inside}\")\n",
    "print(f\"Outside AD: {n_outside}\")\n",
    "\n",
    "# === Append AD Results to Prediction File ===\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "prediction_file = DATA_DIR / f\"{fingerprint}_Analogues_Predictions.csv\"\n",
    "\n",
    "assert prediction_file.exists(), f\"Prediction file not found: {prediction_file}\"\n",
    "\n",
    "final_df = pd.read_csv(prediction_file)\n",
    "\n",
    "# Sanity check alignment\n",
    "if len(final_df) != len(leverage_tq):\n",
    "    raise ValueError(\n",
    "        f\"Row mismatch: prediction file has {len(final_df)} rows \"\n",
    "        f\"but AD array has {len(leverage_tq)} rows.\"\n",
    "    )\n",
    "\n",
    "# Add AD columns\n",
    "final_df[\"Leverage\"] = leverage_tq\n",
    "final_df[\"AD_Flag\"] = ad_flag\n",
    "\n",
    "# Save updated file\n",
    "output_file = DATA_DIR / f\"{fingerprint}_Analogues_Predictions_AD.csv\"\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nAD results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fejYn11vAtkV"
   },
   "source": [
    "**Step 14: Distribution of External Library by Applicability Domain and Activity Class**\n",
    "\n",
    "A bubble plot is generated to visualise the distribution of predicted compounds according to:\n",
    "\n",
    "- Applicability Domain status (Inside / Outside AD)\n",
    "- Activity classification (Inactive / Intermediate / Active)\n",
    "\n",
    "Bubble size and colour reflect the number of compounds in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 1684,
     "status": "ok",
     "timestamp": 1762665388466,
     "user": {
      "displayName": "Jabir Nalicho",
      "userId": "08510785528384725113"
     },
     "user_tz": -180
    },
    "id": "YQBfO3JEAtAp",
    "outputId": "647fc589-82a7-40ee-e6c4-0992ec1550e2"
   },
   "outputs": [],
   "source": [
    "# ---------- Matplotlib Configuration ----------\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from pathlib import Path\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"STIXGeneral\",\n",
    "    \"mathtext.fontset\": \"stix\",\n",
    "    \"mathtext.default\": \"regular\",\n",
    "})\n",
    "\n",
    "# ---------- Load Prediction + AD Dataset ----------\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "file_path = DATA_DIR / f\"{fingerprint}_Analogues_Predictions_AD.csv\"\n",
    "\n",
    "assert file_path.exists(), f\"File not found: {file_path}\"\n",
    "\n",
    "counts = (\n",
    "    pd.read_csv(file_path)\n",
    "    .groupby(['AD_Flag', 'Activity_Class'])\n",
    "    .size()\n",
    "    .reset_index(name='Count')\n",
    ")\n",
    "\n",
    "# ---------- Map Categories to Axes ----------\n",
    "x_map = {'Inside AD': 0, 'Outside AD': 1}\n",
    "y_map = {'Inactive': 0, 'Intermediate': 1, 'Active': 2}\n",
    "\n",
    "counts['x'] = counts['AD_Flag'].map(x_map)\n",
    "counts['y'] = counts['Activity_Class'].map(y_map)\n",
    "\n",
    "# ---------- Bubble Sizes ----------\n",
    "bubble_sizes = counts['Count'] * 20 + 200\n",
    "\n",
    "# ---------- Custom Colormap ----------\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "    'green_blue_red',\n",
    "    ['green', 'blue', 'red']\n",
    ")\n",
    "\n",
    "# ---------- Create Plot ----------\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    counts['x'],\n",
    "    counts['y'],\n",
    "    s=bubble_sizes,\n",
    "    c=counts['Count'],\n",
    "    cmap=cmap,\n",
    "    alpha=0.75,\n",
    "    edgecolors='black',\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# ---------- Annotate Bubbles ----------\n",
    "for _, row in counts.iterrows():\n",
    "    ax.text(\n",
    "        row['x'],\n",
    "        row['y'],\n",
    "        int(row['Count']),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        fontsize=11,\n",
    "        color='white'\n",
    "    )\n",
    "\n",
    "# ---------- Axes Formatting ----------\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 2.5)\n",
    "\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Inside AD', 'Outside AD'], fontsize=14)\n",
    "\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_yticklabels(['Inactive', 'Intermediate', 'Active'], fontsize=14)\n",
    "\n",
    "ax.set_xlabel(r'$\\mathbf{Applicability\\ Domain}$', fontsize=16)\n",
    "ax.set_ylabel(r'$\\mathbf{Activity\\ Class}$', fontsize=16)\n",
    "\n",
    "# ---------- Colorbar ----------\n",
    "cbar = fig.colorbar(scatter, ax=ax, pad=0.02)\n",
    "cbar.set_label('Number of Compounds', fontsize=14)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# ---------- Styling ----------\n",
    "ax.grid(False)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ---------- Save Figure ----------\n",
    "for ext in [\"png\", \"svg\", \"pdf\"]:\n",
    "    fname = DATA_DIR / f\"AD_Activity_Distribution_{fingerprint}.{ext}\"\n",
    "    fig.savefig(fname, dpi=600, bbox_inches='tight', facecolor='white')\n",
    "\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPEUrIO/fFJP+Zkf+ZaRC0k",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
